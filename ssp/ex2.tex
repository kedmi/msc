\documentclass[10pt]{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\begin{document}
\title{Statistical Signal Processing\\Exercise 2}
\author{Sagi Kedmi}
\date{Nov. 21, 2014}
\maketitle
\section{KL Divergence}
Wiki: In probability theory and information theory, the \textbf{Kullback-Leibler divergence} is a non-symmetric measure of the difference between two probability distribution \textit{P} and \textit{Q}. Specifically, the KL-divergence of \textit{Q} from \textit{P} is a measure of the information lost when \textit{Q}%% is used to approximate \textit{P}: The KL divergence measure the expected number of extra bits required to code [Huffman code] samples from \textit{P} when using a code based on \textit{Q}, rather than using the the true code based on \textit{P}. Typically \textit{P} represents the "true" distribution of data, observations, or a precisely calculated theoretical distribution. The measure \textit{Q} typically represents a theory, mode, description or approximation of \textit{P}.
\\\\
In this question we're finding the KL divergence of $f_{2}$ from $f_{1}$.\\\\
Using the Law of the Unconscious Statistician:
\begin{center}
$\mathcal{D}_{KL}\left({f_{1}||f_{2}}\right)=\mathbb{E}_{f_{1}}\left[\mathrm{log}\left({\dfrac{f_{1}(y)}{f_{2}(y)}}\right)\right]={\displaystyle \int_{-\infty}^\infty \mathrm{log}\left({\dfrac{f_{1}(y)}{f_{2}(y)}}\right)f_{1}(y)\,\mathrm{d}y}={\displaystyle \int_{-\infty}^\infty -\mathrm{log}\left({\dfrac{f_{2}(y)}{f_{1}(y)}}\right)f_{1}(y)\,\mathrm{d}y}$
\end{center}
Known inequality: $\forall x\geq 0 : \mathrm{log}(x) \leq x-1$. $f_{1}$ and $f_{2}$ are probability distributions, hence $\dfrac{f_{2}(y)}{f_{1}(y)}\geq 0$. Therefore:
\begin{center}
$\mathcal{D}_{KL}\left({f_{1}||f_{2}}\right)={\displaystyle \int_{-\infty}^\infty -\mathrm{log}\left({\dfrac{f_{2}(y)}{f_{1}(y)}}\right)f_{1}(y)\,\mathrm{d}y}\geq {\displaystyle \int_{-\infty}^\infty \left(-{\dfrac{f_{2}(y)}{f_{1}(y)}} + 1\right)f_{1}(y)\,\mathrm{d}y}={\displaystyle \int_{-\infty}^\infty -f_{2}(y)\,\mathrm{d}y}+{\displaystyle \int_{-\infty}^\infty f_{1}(y)\,\mathrm{d}y}$
\end{center}
\begin{center}
$\mathcal{D}_{KL}\left({f_{1}||f_{2}}\right)\geq -1 +1 = 0.$\\\
$\hfill \blacksquare$
\end{center}
\section{Gaussian Fisher Information}
In an attached document.
\section{ML Estimator Derivation}
In an attached document.

\end{document} 